{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"prefect-spark-on-k8s-operator","text":"<p>Prefect integrations for orchestrating and monitoring apache spark jobs on kubernetes using spark-on-k8s-operator.</p>"},{"location":"#welcome","title":"Welcome!","text":"<p><code>prefect-spark-on-k8s-operator</code> is a collection of Prefect flows enabling orchestration, observation and management of <code>SparkApplication</code> custom kubernetes resources defined according to spark-on-k8s-operator CRD v1Beta2 API Spec.</p> <p>Jump to examples.</p>"},{"location":"#resources","title":"Resources","text":"<p>For more tips on how to use tasks and flows in a Collection, check out Using Collections!</p>"},{"location":"#installation","title":"Installation","text":"<p>You need to configure the kubernetes credentials as per <code>prefect-kubernetes</code> documentation. Install <code>prefect-spark-on-k8s-operator</code> with <code>pip</code>:</p> <pre><code>pip install prefect-spark-on-k8s-operator\n</code></pre> <p>Requires an installation of Python 3.7+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv.</p> <p>These flows are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation.</p>"},{"location":"#example-usage","title":"Example Usage","text":""},{"location":"#specify-and-run-a-sparkapplication-from-a-yaml-file","title":"Specify and run a SparkApplication from a yaml file","text":"<pre><code>import asyncio\n\nfrom prefect_kubernetes.credentials import KubernetesCredentials\nfrom prefect_spark_on_k8s_operator import (\n    SparkApplication,\n    run_spark_application, # this is a flow\n)\n\napp = SparkApplication.from_yaml_file(\n    credentials=KubernetesCredentials.load(\"k8s-creds\"),\n    manifest_path=\"path/to/spark_application.yaml\",\n)\n\n\nif __name__ == \"__main__\":\n    # run the flow\n    asyncio.run(run_spark_application(app))\n</code></pre>"},{"location":"#feedback","title":"Feedback","text":"<p>If you encounter any bugs while using <code>prefect-spark-on-k8s-operator</code>, feel free to open an issue in the prefect-spark-on-k8s-operator repository.</p> <p>If you have any questions or issues while using <code>prefect-spark-on-k8s-operator</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p> <p>Feel free to star or watch <code>prefect-spark-on-k8s-operator</code> for updates too!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you'd like to help contribute to fix an issue or add a feature to <code>prefect-spark-on-k8s-operator</code>, please propose changes through a pull request from a fork of the repository.</p> <p>Here are the steps:</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>"},{"location":"app/","title":"SparkApplication","text":""},{"location":"app/#prefect_spark_on_k8s_operator.app","title":"<code>prefect_spark_on_k8s_operator.app</code>","text":"<p>Module to define SparkApplication and monitor its Run</p>"},{"location":"app/#prefect_spark_on_k8s_operator.app-classes","title":"Classes","text":""},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplication","title":"<code>SparkApplication</code>","text":"<p>         Bases: <code>JobBlock</code></p> <p>A block representing a spark application configuration. The object instance can be created by <code>from_yaml_file</code> classmethod. Below are the additional attributes which can be passed to it if you want to change the <code>SparkApplicationRun</code> behaviour.</p> <p>Attributes:</p> Name Type Description <code>manifest</code> <code>Dict[str, Any]</code> <p>The spark application manifest(spark-on-k8s-operator v1Beta2 API spec) to run. This dictionary can be produced using <code>yaml.safe_load</code>.</p> <code>credentials</code> <code>KubernetesCredentials</code> <p>The credentials to configure a client from.</p> <code>delete_after_completion</code> <code>bool</code> <p>Whether to delete the application after it has completed. Defaults to <code>True</code>.</p> <code>interval_seconds</code> <code>int</code> <p>The number of seconds to wait between application status checks. Defaults to <code>5</code> seconds.</p> <code>namespace</code> <code>str</code> <p>The namespace to create and run the application in. Defaults to <code>default</code>.</p> <code>timeout_seconds</code> <code>Optional[int]</code> <p>The number of seconds to wait for the application in UNKNOWN state before timing out. Defaults to <code>600</code> (10 minutes).</p> <code>collect_driver_logs</code> <code>Optional[bool]</code> <p>Whether to collect driver logs after completion. By default this is done only upon failures. Logs for successful runs will be collected by setting this option to True. Defaults to <code>False</code>.</p> <code>api_kwargs</code> <code>Dict[str, Any]</code> <p>Additional arguments to include in Kubernetes API calls.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>class SparkApplication(JobBlock):\n\"\"\"A block representing a spark application configuration.\n    The object instance can be created by `from_yaml_file` classmethod.\n    Below are the additional attributes which can be passed to it if you want to\n    change the `SparkApplicationRun` behaviour.\n\n    Attributes:\n        manifest:\n            The spark application manifest(spark-on-k8s-operator v1Beta2 API spec)\n            to run. This dictionary can be produced\n            using `yaml.safe_load`.\n        credentials:\n            The credentials to configure a client from.\n        delete_after_completion:\n            Whether to delete the application after it has completed.\n            Defaults to `True`.\n        interval_seconds:\n            The number of seconds to wait between application status checks.\n            Defaults to `5` seconds.\n        namespace:\n            The namespace to create and run the application in. Defaults to `default`.\n        timeout_seconds:\n            The number of seconds to wait for the application in UNKNOWN\n            state before timing out.\n            Defaults to `600` (10 minutes).\n        collect_driver_logs:\n            Whether to collect driver logs after completion.\n            By default this is done only upon failures. Logs for successful runs will\n            be collected by setting this option to True.\n            Defaults to `False`.\n        api_kwargs:\n            Additional arguments to include in Kubernetes API calls.\n    \"\"\"\n\n    # Duplicated description until griffe supports pydantic Fields.\n    manifest: Dict[str, Any] = Field(\n        default=...,\n        title=\"SparkApplication Manifest\",\n        description=(\n            \"The spark application manifest(as per spark-on-k8s-operator API spec)\"\n            \" to run. This dictionary can be produced \"\n            \"using `yaml.safe_load`.\"\n        ),\n    )\n    api_kwargs: Dict[str, Any] = Field(\n        default_factory=dict,\n        title=\"Additional API Arguments\",\n        description=\"Additional arguments to include in Kubernetes API calls.\",\n        example={\"pretty\": \"true\"},\n    )\n    credentials: KubernetesCredentials = Field(\n        default=..., description=\"The credentials to configure a client from.\"\n    )\n    delete_after_completion: bool = Field(\n        default=True,\n        description=\"Whether to delete the application after it has completed.\",\n    )\n    interval_seconds: int = Field(\n        default=5,\n        description=\"The number of seconds to wait between application status checks.\",\n    )\n    namespace: str = Field(\n        default=\"default\",\n        description=\"The namespace to create and run the application in.\",\n    )\n    timeout_seconds: Optional[int] = Field(\n        default=600,\n        description=\"The number of seconds to wait for the application in \"\n        \"UNKNOWN state before timing out.\",\n    )\n    collect_driver_logs: Optional[bool] = Field(\n        default=False,\n        description=(\n            \"Whether to collect driver logs after completion.\"\n            \" By default this is done only upon failures.\"\n            \" Logs for successful runs will be collected by setting this option to True\"\n        ),\n    )\n\n    _block_type_name = \"Spark On K8s Operator\"\n    _block_type_slug = \"spark-on-k8s-operator\"\n    _logo_url = \"https://images.ctfassets.net/zscdif0zqppk/35vNcprr3MmIlkrKxxCiah/1d720b4b50dfa8876198cf21730cf123/Kubernetes_logo_without_workmark.svg.png?h=250\"  # noqa: E501\n    _documentation_url = \"https://tardunge.github.io/prefect-spark-on-k8s-operator/app/#prefect_spark_on_k8s_operator.app.SparkApplication\"  # noqa\n    name: str = \"\"\n\n    @sync_compatible\n    async def trigger(self) -&gt; \"SparkApplicationRun\":\n\"\"\"Apply the spark application and return a `SparkApplicationRun` object.\n\n        Returns:\n            SparkApplicationRun object.\n        \"\"\"\n\n        # randomize the application run instance name.\n        name = (\n            self.manifest.get(constants.METADATA).get(constants.NAME)\n            + \"-\"\n            + \"\".join(random.choices(string.ascii_lowercase + string.digits, k=4))\n        )\n        self.manifest.get(constants.METADATA)[constants.NAME] = name\n\n        manifest = await create_namespaced_custom_object.fn(\n            kubernetes_credentials=self.credentials,\n            group=constants.GROUP,\n            version=constants.VERSION,\n            plural=constants.PLURAL,\n            body=self.manifest,\n            namespace=self.namespace,\n            **self.api_kwargs,\n        )\n        self.logger.info(\n            \"Created spark application: \"\n            f\"{manifest.get(constants.METADATA).get(constants.NAME)}\"\n        )\n\n        self.manifest = manifest\n        self.name = manifest.get(constants.METADATA).get(constants.NAME)\n        return SparkApplicationRun(spark_application=self)\n\n    @classmethod\n    def from_yaml_file(\n        cls: Type[Self], manifest_path: Union[Path, str], **kwargs\n    ) -&gt; Self:\n\"\"\"Create a `SparkApplication` from a YAML file.\n        Supports manifests of SparkApplication or ScheduledSparkApplication Kind only.\n        If a `ScheduledSparkApplication` is provided, it is converted to a\n        `SparkApplication` Kind. It forcefully sets the restartPolicy of the application\n        to 'Never'.\n\n        Args:\n            manifest_path: The YAML file to create the `SparkApplication` from.\n\n        Returns:\n            A SparkApplicationRun object.\n        \"\"\"\n        with open(manifest_path, \"r\") as yaml_stream:\n            yaml_dict = yaml.safe_load(yaml_stream)\n\n        # convert ScheduledSparkApplication to SparkApplication\n        # as the schedules are handled by prefect.\n        if yaml_dict.get(constants.KIND) == constants.SCHEDULED_SPARK_APPLICATION_KIND:\n            yaml_dict[constants.KIND] = constants.SPARK_APPLICATION_KIND\n            yaml_dict[constants.SPEC] = yaml_dict[constants.SPEC][constants.TEMPLATE]\n\n        if (\n            yaml_dict.get(constants.KIND) not in constants.SPARK_APPLICATION_KINDS\n            or yaml_dict.get(constants.SPEC).get(constants.TYPE)\n            not in constants.SPARK_APPLICATION_TYPES\n        ):\n            raise TypeError(\n                \"The provided manifest has either unsupport kind or spec.type\"\n            )\n\n        # Forcefully set restartPolicy to Never. Schedules and retries\n        # should be dictated by prefect flow.\n        yaml_dict.get(constants.SPEC).update(constants.RESTART_POLICY)\n        return cls(manifest=yaml_dict, **kwargs)\n</code></pre>"},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplication-functions","title":"Functions","text":""},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplication.from_yaml_file","title":"<code>from_yaml_file</code>  <code>classmethod</code>","text":"<p>Create a <code>SparkApplication</code> from a YAML file. Supports manifests of SparkApplication or ScheduledSparkApplication Kind only. If a <code>ScheduledSparkApplication</code> is provided, it is converted to a <code>SparkApplication</code> Kind. It forcefully sets the restartPolicy of the application to 'Never'.</p> <p>Parameters:</p> Name Type Description Default <code>manifest_path</code> <code>Union[Path, str]</code> <p>The YAML file to create the <code>SparkApplication</code> from.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A SparkApplicationRun object.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>@classmethod\ndef from_yaml_file(\n    cls: Type[Self], manifest_path: Union[Path, str], **kwargs\n) -&gt; Self:\n\"\"\"Create a `SparkApplication` from a YAML file.\n    Supports manifests of SparkApplication or ScheduledSparkApplication Kind only.\n    If a `ScheduledSparkApplication` is provided, it is converted to a\n    `SparkApplication` Kind. It forcefully sets the restartPolicy of the application\n    to 'Never'.\n\n    Args:\n        manifest_path: The YAML file to create the `SparkApplication` from.\n\n    Returns:\n        A SparkApplicationRun object.\n    \"\"\"\n    with open(manifest_path, \"r\") as yaml_stream:\n        yaml_dict = yaml.safe_load(yaml_stream)\n\n    # convert ScheduledSparkApplication to SparkApplication\n    # as the schedules are handled by prefect.\n    if yaml_dict.get(constants.KIND) == constants.SCHEDULED_SPARK_APPLICATION_KIND:\n        yaml_dict[constants.KIND] = constants.SPARK_APPLICATION_KIND\n        yaml_dict[constants.SPEC] = yaml_dict[constants.SPEC][constants.TEMPLATE]\n\n    if (\n        yaml_dict.get(constants.KIND) not in constants.SPARK_APPLICATION_KINDS\n        or yaml_dict.get(constants.SPEC).get(constants.TYPE)\n        not in constants.SPARK_APPLICATION_TYPES\n    ):\n        raise TypeError(\n            \"The provided manifest has either unsupport kind or spec.type\"\n        )\n\n    # Forcefully set restartPolicy to Never. Schedules and retries\n    # should be dictated by prefect flow.\n    yaml_dict.get(constants.SPEC).update(constants.RESTART_POLICY)\n    return cls(manifest=yaml_dict, **kwargs)\n</code></pre>"},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplication.trigger","title":"<code>trigger</code>  <code>async</code>","text":"<p>Apply the spark application and return a <code>SparkApplicationRun</code> object.</p> <p>Returns:</p> Type Description <code>SparkApplicationRun</code> <p>SparkApplicationRun object.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>@sync_compatible\nasync def trigger(self) -&gt; \"SparkApplicationRun\":\n\"\"\"Apply the spark application and return a `SparkApplicationRun` object.\n\n    Returns:\n        SparkApplicationRun object.\n    \"\"\"\n\n    # randomize the application run instance name.\n    name = (\n        self.manifest.get(constants.METADATA).get(constants.NAME)\n        + \"-\"\n        + \"\".join(random.choices(string.ascii_lowercase + string.digits, k=4))\n    )\n    self.manifest.get(constants.METADATA)[constants.NAME] = name\n\n    manifest = await create_namespaced_custom_object.fn(\n        kubernetes_credentials=self.credentials,\n        group=constants.GROUP,\n        version=constants.VERSION,\n        plural=constants.PLURAL,\n        body=self.manifest,\n        namespace=self.namespace,\n        **self.api_kwargs,\n    )\n    self.logger.info(\n        \"Created spark application: \"\n        f\"{manifest.get(constants.METADATA).get(constants.NAME)}\"\n    )\n\n    self.manifest = manifest\n    self.name = manifest.get(constants.METADATA).get(constants.NAME)\n    return SparkApplicationRun(spark_application=self)\n</code></pre>"},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplicationRun","title":"<code>SparkApplicationRun</code>","text":"<p>         Bases: <code>JobRun[Dict[str, Any]]</code></p> <p>A container representing a run of a spark application.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>class SparkApplicationRun(JobRun[Dict[str, Any]]):\n\"\"\"A container representing a run of a spark application.\"\"\"\n\n    def __init__(\n        self,\n        spark_application: \"SparkApplication\",\n    ):\n        self.application_logs = None\n\n        self._completed = False\n        self._timed_out = False\n        self._terminal_state = \"\"\n        self._error_msg = \"The Run has not encounterend any errors.\"\n        self._spark_application = spark_application\n        self._status = None\n        self._cleanup_status = False\n\n    async def _cleanup(self) -&gt; bool:\n\"\"\"Deletes the resources created by the spark application.\n        Produces Resourceleak warning in case the delete was unsuccessful.\n        \"\"\"\n        deleted = await delete_namespaced_custom_object.fn(\n            kubernetes_credentials=self._spark_application.credentials,\n            group=constants.GROUP,\n            version=constants.VERSION,\n            plural=constants.PLURAL,\n            name=self._spark_application.name,\n            namespace=self._spark_application.namespace,\n            **self._spark_application.api_kwargs,\n        )\n        status = deleted.get(constants.STATUS) == constants.SUCCESS\n        if status:\n            self.logger.info(\n                \"cleaned up resources for app: \"\n                f\"{deleted.get(constants.DETAILS).get(constants.NAME)}\"\n            )\n        else:\n            self.logger.warning(\n                f\"Resource leak: failed to clean up, {self._spark_application.name}\",\n            )\n\n        return status\n\n    async def _fetch_status(self) -&gt; Dict[str, Any]:\n\"\"\"Reads the runtime status of the spark application.\"\"\"\n        self._status = await get_namespaced_custom_object_status.fn(\n            kubernetes_credentials=self._spark_application.credentials,\n            group=constants.GROUP,\n            version=constants.VERSION,\n            plural=constants.PLURAL,\n            name=self._spark_application.name,\n            namespace=self._spark_application.namespace,\n            **self._spark_application.api_kwargs,\n        )\n        return self._status\n\n    @sync_compatible\n    async def wait_for_completion(self):\n\"\"\"Waits for the application to reach a terminal state.\n        The terminal states currently used are:\n        COMPLETED:\n            The application has run successfully.\n        FAILED:\n            The application has encounterend some error.\n        UNKNOWN:\n            The application is in UNKNOWN state. This happens if the\n            Kubernetes node hosting the driver pod crashes. If the\n            state doesn't change for `timeout_seconds`, then the application\n            is terminated.\n        For more information on the spark-on-k8s-operator state-machine please\n        refer [here](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/\n        blob/master/pkg/controller/sparkapplication/controller.go#L485)\n\n        Raises:\n            RuntimeError: If the application fails or in unknown state\n                for timeout_seconds.\n        \"\"\"\n        self.application_logs = {}\n\n        # wait for the status to change from \"\"(empty string) to something.\n        while True:\n            status = await self._fetch_status()\n            if constants.STATUS not in status:\n                await sleep(1)\n                continue\n            break\n\n        # wait for the application to reach a terminal_state\n        # which is either COMPLETED or FAILED\n        # or UNKNOWN(for timeout_seconds)\n        while not self._completed:\n            status = await self._fetch_status()\n            app_state = (\n                status.get(constants.STATUS)\n                .get(constants.APPLICATION_STATE)\n                .get(constants.STATE)\n            )\n            self.logger.info(f\"Last obeserved heartbeat: {app_state}\")\n            if app_state in [constants.COMPLETED, constants.FAILED]:\n                self._completed = True\n                self._terminal_state = app_state\n                self.logger.info(f\"{self._completed}\")\n                self.logger.info(f\"{self._terminal_state}\")\n\n            # happens when node/kubelet crashes.\n            # Stop the application if this state doesn't change until timeout_seconds.\n            elif app_state == constants.UNKNOWN:\n                timer_start = int(perf_counter())\n                while not self._timed_out:\n                    await sleep(self._spark_application.interval_seconds)\n                    status = await self._fetch_status()\n                    app_state = (\n                        status.get(constants.STATUS)\n                        .get(constants.APPLICATION_STATE)\n                        .get(constants.STATE)\n                    )\n                    if app_state != constants.UNKNOWN:\n                        timer_start = 0\n                        break\n                    if (\n                        int(perf_counter()) - timer_start\n                        &gt; self._spark_application.timeout_seconds\n                    ):\n                        self._completed = True\n                        self._timed_out = True\n                        self._terminal_state = app_state\n            else:\n                await sleep(self._spark_application.interval_seconds)\n\n        # restore the value after getting rid of loops.\n        if self._terminal_state != constants.COMPLETED:\n            self._completed = False\n\n        _tail_logs = False\n        if self._terminal_state == constants.FAILED:\n            _tail_logs = True\n        if self._spark_application.collect_driver_logs:\n            _tail_logs = True\n        if self._terminal_state == constants.UNKNOWN:\n            _tail_logs = False\n\n        if _tail_logs:\n            self._error_msg = (\n                self._status.get(constants.STATUS)\n                .get(constants.APPLICATION_STATE)\n                .get(constants.ERROR_MESSAGE, self._error_msg)\n            )\n            app_id = self._status.get(constants.STATUS).get(\n                constants.SPARK_APPLICATION_ID\n            )\n            app_submission_id = self._status.get(constants.STATUS).get(\n                constants.SUBMISSION_ID\n            )\n\n            v1_pod_list = await list_namespaced_pod.fn(\n                kubernetes_credentials=self._spark_application.credentials,\n                namespace=self._spark_application.namespace,\n                label_selector=generate_pod_selectors(\n                    self._spark_application.name,\n                    app_id,\n                    app_submission_id,\n                ),\n                **self._spark_application.api_kwargs,\n            )\n            self.logger.info(f\"pod_list: {len(v1_pod_list.items)}\")\n            for pod in v1_pod_list.items:\n                pod_name = pod.metadata.name\n                self.logger.info(f\"Capturing logs for pod {pod_name!r}.\")\n                self.application_logs[pod_name] = await read_namespaced_pod_log.fn(\n                    kubernetes_credentials=self._spark_application.credentials,\n                    namespace=self._spark_application.namespace,\n                    pod_name=pod_name,\n                    container=constants.SPARK_DRIVER_CONAINER_NAME,\n                    **self._spark_application.api_kwargs,\n                )\n\n        if self._spark_application.delete_after_completion or self._timed_out:\n            self._cleanup_status = await self._cleanup()\n\n        if self._terminal_state != constants.COMPLETED:\n            raise RuntimeError(\n                \"The SparkApplication run is not in a completed state,\"\n                f\" last observed state was {self._terminal_state} - \"\n                f\"possible errors: {self._error_msg}\"\n            )\n\n    @sync_compatible\n    async def fetch_result(self) -&gt; Dict[str, Any]:\n\"\"\"Returns the logs from driver pod when:\n        `collect_driver_logs` is set to true\n        or the application is not in COMPLETED state.\n\n        Returns:\n            A dict containing the driver pod name and its main container logs.\n        \"\"\"\n        self.logger.info(f\"logs: {self.application_logs}\")\n        if self._terminal_state != constants.COMPLETED:\n            raise ValueError(\n                \"The SparkApplication run is not in a completed state,\"\n                f\" last observed state was {self._terminal_state} - \"\n                f\"possible errors: {self._error_msg}\"\n            )\n        return self.application_logs\n</code></pre>"},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplicationRun-functions","title":"Functions","text":""},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplicationRun.fetch_result","title":"<code>fetch_result</code>  <code>async</code>","text":"<p>Returns the logs from driver pod when: <code>collect_driver_logs</code> is set to true or the application is not in COMPLETED state.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dict containing the driver pod name and its main container logs.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>@sync_compatible\nasync def fetch_result(self) -&gt; Dict[str, Any]:\n\"\"\"Returns the logs from driver pod when:\n    `collect_driver_logs` is set to true\n    or the application is not in COMPLETED state.\n\n    Returns:\n        A dict containing the driver pod name and its main container logs.\n    \"\"\"\n    self.logger.info(f\"logs: {self.application_logs}\")\n    if self._terminal_state != constants.COMPLETED:\n        raise ValueError(\n            \"The SparkApplication run is not in a completed state,\"\n            f\" last observed state was {self._terminal_state} - \"\n            f\"possible errors: {self._error_msg}\"\n        )\n    return self.application_logs\n</code></pre>"},{"location":"app/#prefect_spark_on_k8s_operator.app.SparkApplicationRun.wait_for_completion","title":"<code>wait_for_completion</code>  <code>async</code>","text":"<p>Waits for the application to reach a terminal state. The terminal states currently used are:</p> COMPLETED <p>The application has run successfully.</p> FAILED <p>The application has encounterend some error.</p> UNKNOWN <p>The application is in UNKNOWN state. This happens if the Kubernetes node hosting the driver pod crashes. If the state doesn't change for <code>timeout_seconds</code>, then the application is terminated.</p> <p>For more information on the spark-on-k8s-operator state-machine please refer here</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the application fails or in unknown state for timeout_seconds.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>@sync_compatible\nasync def wait_for_completion(self):\n\"\"\"Waits for the application to reach a terminal state.\n    The terminal states currently used are:\n    COMPLETED:\n        The application has run successfully.\n    FAILED:\n        The application has encounterend some error.\n    UNKNOWN:\n        The application is in UNKNOWN state. This happens if the\n        Kubernetes node hosting the driver pod crashes. If the\n        state doesn't change for `timeout_seconds`, then the application\n        is terminated.\n    For more information on the spark-on-k8s-operator state-machine please\n    refer [here](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/\n    blob/master/pkg/controller/sparkapplication/controller.go#L485)\n\n    Raises:\n        RuntimeError: If the application fails or in unknown state\n            for timeout_seconds.\n    \"\"\"\n    self.application_logs = {}\n\n    # wait for the status to change from \"\"(empty string) to something.\n    while True:\n        status = await self._fetch_status()\n        if constants.STATUS not in status:\n            await sleep(1)\n            continue\n        break\n\n    # wait for the application to reach a terminal_state\n    # which is either COMPLETED or FAILED\n    # or UNKNOWN(for timeout_seconds)\n    while not self._completed:\n        status = await self._fetch_status()\n        app_state = (\n            status.get(constants.STATUS)\n            .get(constants.APPLICATION_STATE)\n            .get(constants.STATE)\n        )\n        self.logger.info(f\"Last obeserved heartbeat: {app_state}\")\n        if app_state in [constants.COMPLETED, constants.FAILED]:\n            self._completed = True\n            self._terminal_state = app_state\n            self.logger.info(f\"{self._completed}\")\n            self.logger.info(f\"{self._terminal_state}\")\n\n        # happens when node/kubelet crashes.\n        # Stop the application if this state doesn't change until timeout_seconds.\n        elif app_state == constants.UNKNOWN:\n            timer_start = int(perf_counter())\n            while not self._timed_out:\n                await sleep(self._spark_application.interval_seconds)\n                status = await self._fetch_status()\n                app_state = (\n                    status.get(constants.STATUS)\n                    .get(constants.APPLICATION_STATE)\n                    .get(constants.STATE)\n                )\n                if app_state != constants.UNKNOWN:\n                    timer_start = 0\n                    break\n                if (\n                    int(perf_counter()) - timer_start\n                    &gt; self._spark_application.timeout_seconds\n                ):\n                    self._completed = True\n                    self._timed_out = True\n                    self._terminal_state = app_state\n        else:\n            await sleep(self._spark_application.interval_seconds)\n\n    # restore the value after getting rid of loops.\n    if self._terminal_state != constants.COMPLETED:\n        self._completed = False\n\n    _tail_logs = False\n    if self._terminal_state == constants.FAILED:\n        _tail_logs = True\n    if self._spark_application.collect_driver_logs:\n        _tail_logs = True\n    if self._terminal_state == constants.UNKNOWN:\n        _tail_logs = False\n\n    if _tail_logs:\n        self._error_msg = (\n            self._status.get(constants.STATUS)\n            .get(constants.APPLICATION_STATE)\n            .get(constants.ERROR_MESSAGE, self._error_msg)\n        )\n        app_id = self._status.get(constants.STATUS).get(\n            constants.SPARK_APPLICATION_ID\n        )\n        app_submission_id = self._status.get(constants.STATUS).get(\n            constants.SUBMISSION_ID\n        )\n\n        v1_pod_list = await list_namespaced_pod.fn(\n            kubernetes_credentials=self._spark_application.credentials,\n            namespace=self._spark_application.namespace,\n            label_selector=generate_pod_selectors(\n                self._spark_application.name,\n                app_id,\n                app_submission_id,\n            ),\n            **self._spark_application.api_kwargs,\n        )\n        self.logger.info(f\"pod_list: {len(v1_pod_list.items)}\")\n        for pod in v1_pod_list.items:\n            pod_name = pod.metadata.name\n            self.logger.info(f\"Capturing logs for pod {pod_name!r}.\")\n            self.application_logs[pod_name] = await read_namespaced_pod_log.fn(\n                kubernetes_credentials=self._spark_application.credentials,\n                namespace=self._spark_application.namespace,\n                pod_name=pod_name,\n                container=constants.SPARK_DRIVER_CONAINER_NAME,\n                **self._spark_application.api_kwargs,\n            )\n\n    if self._spark_application.delete_after_completion or self._timed_out:\n        self._cleanup_status = await self._cleanup()\n\n    if self._terminal_state != constants.COMPLETED:\n        raise RuntimeError(\n            \"The SparkApplication run is not in a completed state,\"\n            f\" last observed state was {self._terminal_state} - \"\n            f\"possible errors: {self._error_msg}\"\n        )\n</code></pre>"},{"location":"app/#prefect_spark_on_k8s_operator.app-functions","title":"Functions","text":""},{"location":"app/#prefect_spark_on_k8s_operator.app.generate_pod_selectors","title":"<code>generate_pod_selectors</code>","text":"<p>Generates pod selector labels given the name, app_id and submission_id of the spark application.</p> Source code in <code>prefect_spark_on_k8s_operator/app.py</code> <pre><code>def generate_pod_selectors(name: str, app_id: str, submission_id: str) -&gt; List[str]:\n\"\"\"Generates pod selector labels given the\n    name, app_id and submission_id of the spark application.\n    \"\"\"\n    template = string.Template(constants.LABELS_TEMPLATE)\n    labels = template.safe_substitute(\n        name=name, app_id=app_id, submission_id=submission_id\n    )\n    return labels\n</code></pre>"},{"location":"flows/","title":"Flows","text":""},{"location":"flows/#prefect_spark_on_k8s_operator.flows","title":"<code>prefect_spark_on_k8s_operator.flows</code>","text":"<p>A module to define flows interacting with spark-on-k8s-operator resources created in Kubernetes.</p>"},{"location":"flows/#prefect_spark_on_k8s_operator.flows-classes","title":"Classes","text":""},{"location":"flows/#prefect_spark_on_k8s_operator.flows-functions","title":"Functions","text":""},{"location":"flows/#prefect_spark_on_k8s_operator.flows.run_spark_application","title":"<code>run_spark_application</code>  <code>async</code>","text":"<p>Flow for running a spark application using spark-on-k8s-operator.</p> <p>Parameters:</p> Name Type Description Default <code>spark_application</code> <code>SparkApplication</code> <p>The <code>SparkApplication</code> block that specifies the application run params.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The a dict of logs from driver pod after the application reached a terminal state which can be COMPLETED, FAILED, UNKNOWN (for <code>time_out seconds</code>).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the created spark application attains a failed/unknown status.</p> Example <pre><code>import asyncio\n\nfrom prefect_kubernetes.credentials import KubernetesCredentials\nfrom prefect_spark_on_k8s_operator import (\n    SparkApplication,\n    run_spark_application,\n)\n\napp = SparkApplication.from_yaml_file(\n    credentials=KubernetesCredentials.load(\"k8s-creds\"),\n    manifest_path=\"path/to/job.yaml\",\n)\n\n\nif __name__ == \"__main__\":\n    # run the flow\n    asyncio.run(run_spark_application(app))\n</code></pre> Source code in <code>prefect_spark_on_k8s_operator/flows.py</code> <pre><code>@flow\nasync def run_spark_application(\n    spark_application: SparkApplication,\n) -&gt; Dict[str, Any]:\n\"\"\"Flow for running a spark application using spark-on-k8s-operator.\n\n    Args:\n        spark_application: The `SparkApplication` block that specifies the\n            application run params.\n\n    Returns:\n        The a dict of logs from driver pod after the application reached a\n            terminal state which can be COMPLETED, FAILED, UNKNOWN\n            (for `time_out seconds`).\n\n    Raises:\n        RuntimeError: If the created spark application attains a failed/unknown status.\n\n    Example:\n\n        ```python\n        import asyncio\n\n        from prefect_kubernetes.credentials import KubernetesCredentials\n        from prefect_spark_on_k8s_operator import (\n            SparkApplication,\n            run_spark_application,\n        )\n\n        app = SparkApplication.from_yaml_file(\n            credentials=KubernetesCredentials.load(\"k8s-creds\"),\n            manifest_path=\"path/to/job.yaml\",\n        )\n\n\n        if __name__ == \"__main__\":\n            # run the flow\n            asyncio.run(run_spark_application(app))\n        ```\n    \"\"\"\n    spark_application_run = await task(spark_application.trigger.aio)(spark_application)\n\n    await task(spark_application_run.wait_for_completion.aio)(spark_application_run)\n\n    return await task(spark_application_run.fetch_result.aio)(spark_application_run)\n</code></pre>"}]}